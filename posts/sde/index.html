<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Stochastic Differential Equations and Diffusion Models" /><meta name="author" content="Tanmaya Shekhar Dabral" /><meta property="og:locale" content="en_US" /><meta name="description" content="Diffusion models (Sohl-Dickstein et al., 2015)(Ho et al., 2020) are one of the freshest flavors of generative models in the market right now (at least as of writing this post). They have been shown to outperform GANs in certain settings (Dhariwal &amp; Nichol, 2021), and once trained, can also be used as feature extractors for supervised tasks (Baranchuk et al., 2021). As we shall see, they are also very elegant. The models are trained to invert a particular corruption process which corrupts a target distribution (say, the distribution of the images in your dataset) to approximately Gaussian noise. Once the inversion has been learnt, you can generate new samples from the target distribution by sampling Gaussian noise and passing it through the inversion process." /><meta property="og:description" content="Diffusion models (Sohl-Dickstein et al., 2015)(Ho et al., 2020) are one of the freshest flavors of generative models in the market right now (at least as of writing this post). They have been shown to outperform GANs in certain settings (Dhariwal &amp; Nichol, 2021), and once trained, can also be used as feature extractors for supervised tasks (Baranchuk et al., 2021). As we shall see, they are also very elegant. The models are trained to invert a particular corruption process which corrupts a target distribution (say, the distribution of the images in your dataset) to approximately Gaussian noise. Once the inversion has been learnt, you can generate new samples from the target distribution by sampling Gaussian noise and passing it through the inversion process." /><link rel="canonical" href="https://www.vanillabug.com/posts/sde/" /><meta property="og:url" content="https://www.vanillabug.com/posts/sde/" /><meta property="og:site_name" content="VanillaBug" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-09-28T08:31:00-04:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Stochastic Differential Equations and Diffusion Models" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"headline":"Stochastic Differential Equations and Diffusion Models","dateModified":"2021-09-28T08:31:00-04:00","datePublished":"2021-09-28T08:31:00-04:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.vanillabug.com/posts/sde/"},"url":"https://www.vanillabug.com/posts/sde/","author":{"@type":"Person","name":"Tanmaya Shekhar Dabral"},"description":"Diffusion models (Sohl-Dickstein et al., 2015)(Ho et al., 2020) are one of the freshest flavors of generative models in the market right now (at least as of writing this post). They have been shown to outperform GANs in certain settings (Dhariwal &amp; Nichol, 2021), and once trained, can also be used as feature extractors for supervised tasks (Baranchuk et al., 2021). As we shall see, they are also very elegant. The models are trained to invert a particular corruption process which corrupts a target distribution (say, the distribution of the images in your dataset) to approximately Gaussian noise. Once the inversion has been learnt, you can generate new samples from the target distribution by sampling Gaussian noise and passing it through the inversion process.","@context":"https://schema.org"}</script><title>Stochastic Differential Equations and Diffusion Models | VanillaBug</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="VanillaBug"><meta name="application-name" content="VanillaBug"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { packages: {'[+]': ['base', 'ams', 'boldsymbol']}, inlineMath: [['$', '$'], ['\\(', '\\)']] }, loader: { load: ['ui/menu', '[tex]/ams', '[tex]/boldsymbol'] }, startup: { ready() { MathJax.startup.defaultReady(); const Macro = MathJax._.input.tex.Symbol.Macro; const MapHandler = MathJax._.input.tex.MapHandler.MapHandler; const Array = MathJax._.input.tex.ams.AmsMethods.default.Array; const env = new Macro('psmallmatrix', Array, [null, '(', ')', 'c', '.333em', '.2em', 'S', 1]); MapHandler.getMap('AMSmath-environment').add('psmallmatrix', env); } } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-221509888-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-221509888-1'); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">VanillaBug</a></div><div class="site-subtitle font-italic">A website for things.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/many-facedgod" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['tanmaya.dabral','gmail.com'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/tdabral" aria-label="linkedin" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { this.setDark(); } else { this.setLight(); } } else { this.setLight(); } } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.hasMode && this.isDarkMode){ return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isLightMode){ this.setDark(); } else { this.setLight(); } } else { this.setDark(); } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); if (!toggle.hasMode){ toggle.setLight(); } $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Stochastic Differential Equations and Diffusion Models</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Stochastic Differential Equations and Diffusion Models</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Tanmaya Shekhar Dabral </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Sep 28, 2021, 8:31 AM -0400" prep="on" > Sep 28, 2021 <i class="unloaded">2021-09-28T08:31:00-04:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="6903 words">98 min</span></div></div><div class="post-content"><p>Diffusion models <a class="citation" href="#sohl2015deep">(Sohl-Dickstein et al., 2015)</a><a class="citation" href="#ho2020denoising">(Ho et al., 2020)</a> are one of the freshest flavors of generative models in the market right now (at least as of writing this post). They have been shown to outperform GANs in certain settings <a class="citation" href="#dhariwal2021diffusion">(Dhariwal &amp; Nichol, 2021)</a>, and once trained, can also be used as feature extractors for supervised tasks <a class="citation" href="#baranchuk2021label">(Baranchuk et al., 2021)</a>. As we shall see, they are also very elegant. The models are trained to invert a particular corruption process which corrupts a target distribution (say, the distribution of the images in your dataset) to approximately Gaussian noise. Once the inversion has been learnt, you can generate new samples from the target distribution by sampling Gaussian noise and passing it through the inversion process.</p><p>In particular, the diffusion models use a multi-step corruption process where, at each step, the input is scaled down (multiplied by a number less than one, not an image resize) and added with some zero-mean Gaussian noise. That is to say, the output of each step is a sample from a Gaussian distribution with its mean being a down-scaled version of the input. It is reasonable then, that the eventual distribution after many such steps would resemble a Gaussian. In this process, the scaling factor and the variance (or the covariance matrix in case of multi-dimensional data) are only a function of the step number.</p><p>The inversion process is defined in a manner similar to the corruption process, and goes from the last step to the first. The output of an inversion step is defined as a sample from a Gaussian distribution, but now, instead of the mean being a scaled version of the input and the variance being a (step number dependent) constant, they are defined using neural networks which take in the input and the step number and produce the two parameters.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/img/2021-11-11-sde/shrek.jpg" alt="Shrek" title="I may or may not have taken some artistic liberties." width="40%" /></p><p><em>Given</em> this corruption and inversion model, variational inference can be used to learn the parameters of the neural networks involved. In particular, the corrupted variants obtained through the corruption process can be taken to be latent variables for which the posterior distribution is easily obtained. The corruption process then, can be taken to be the proposal distribution and an unbiased estimator for the evidence lower bound can be obtained (there are some algebraic tricks used, as in a variational autoencoder, to reduce the dependence on Monte Carlo estimates by using closed-form KL-divergences between Gaussians). More details can be found in this excellent blog posts: <a class="citation" href="#weng2021diffusion">(Weng, 2021)</a>.</p><p>However, it is not immediately clear <em>why</em> these corruption and inversion processes are defined the way they are. Thankfully, an outstanding recent paper <a class="citation" href="#song2020score">(Song et al., 2020)</a> sheds a bit more light by relating the models to stochastic differential equations. In this blog post, we shall build up to the illustration in that paper from the ground up, armed with nothing but the Taylor approximation, the central limit theorem, our indomitable intuition and an abject disregard for rigour.</p><h2 id="discrete-time-markov-processes">Discrete-Time Markov Processes</h2><p>Let us start with getting a feel for Markov processes with discrete time and a discrete state space. Feel free to skip this section if you’re already familiar with them. Such a Markov Process is defined as a sequence of random variables $X_0, X_1, X_2, \dots$ with the property that given a particular random variable, the ones to its left are independent of the ones to its right. That is:</p>\[X_0, X_1, \dots, X_{i - 1} \perp\!\!\!\!\perp X_{i + 1}, X_{i + 2}, \dots | X_{i}\]<p>As a directed graphical model it looks like:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/img/2021-11-11-sde/pgm.jpg" alt="PGM" title="pls ignore the asymmetries" width="30%" /></p><p>These random variables can be taken to represent the state of a system as it evolves stochastically through time, with the subscripts representing the timestamp. Thus, the Markov property, as defined above, seems a pretty natural one: it says that the future and the past are independent given the present. Based on this, the probability of an observed sequence $x_0, x_1, x_2, \dots$ can be neatly decomposed as:</p>\[P(X_0 = x_0, X_1 = x_1, X_2 = x_2, \dots) = P(X_0 = x_0)\prod_{t \in \{1, 2, \dots\}}P(X_t = x_t | X_{t - 1} = x_{t - 1})\]<p>Indeed, such a Markov process can be completely defined by the distribution of the initial state, $P(X_0 = x_0)$, and the transition probabilities $P(X_t = x_t | X_{t - 1} = x_{t - 1})$. Note how the latter is implicitly a function of $t$ because of the subscript. The transition probabilities from one state to another can be different for different times. We can also get the following intuitive recursion for the process fairly easily:</p>\[P(X_{t} = x_{t} | X_{s} = x_{s}) = \sum_{k} P(X_{t} = x_{t} | X_{m} = k)P(X_{m} = k | X_{s} = x_{s})\]<p>for $t \geq m \geq s.$ This follows from the fact that if we fix the state we encounter at time $m$ as $k,$ the probability of going from $x_s$ at time $s$ to $x_t$ at time $t$ is the product of the probability of getting to $k$ at time $m$ first and then continuing on to $x_t$ at time $t$ because of the independence assumption. Then, we can sum over all possible values of $k$ to marginalize it out (the derivation is fairly straightforward algebraically as well). In fact, in a similar manner we can also write an equation for the unconditional Marginal probability for $X_t.$ Note how this equation does not depend on the Markov property at all:</p>\[P(X_{t} = x_{t}) = \sum_{k} P(X_{t} = x_{t} | X_{m} = k)P(X_{m} = k)\]<p>Finally, it is trivial to see that if $X_0, X_1, \dots, X_{T}$ is a Markov process, then so is its time-reversed version $X_T, X_{t - 1}, \dots, X_0$ since the past being independent of the future given the present is the same as the future being independent of the past given the present.</p><h2 id="continuous-time-markov-processes">Continuous-Time Markov Processes</h2><p>Now that we have seen markov processes where the time is indexed by discrete natural numbers, we can generalize it to the continuous case. To build our intuition, let us recall how we generalize discrete functions to continuous ones:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/img/2021-11-11-sde/discrete_approx.gif" alt="Discretization" width="40%" /></p><p>We increase the number of segments toward infinity by decreasing their width to nigh $0.$ However, intuitively, we also need to compensate for this infinite number of segments somehow to make sure the output of the function remains finite. We do so by decreasing the change in the function value for consecutive segments to almost zero as well.</p><p>We now want to divide the time axis of our Markov process in a manner similar to what we did with the x-axis. So how do we come up with such a compensation in the case of Markov processes? It turns out there are <em>two</em> ways to do so.</p><h3 id="jump-processes">Jump Processes</h3><p>The first way to maintain a sense of finiteness is by making the probability of state transition in an infinitesimal time segment $\mathrm{d}t$ infinitesimally small. This way, over a finite time period, there will be some finite probability of state transition. An example of this is the <a href="https://en.wikipedia.org/wiki/Poisson_point_process">Poisson process</a> which counts the number of random events in a given time. The process assumes that in an infinitesimal time segment $\mathrm{d}t,$ the probability of an event occurring is $\eta\mathrm{d}t$ with $\eta$ being the rate parameter of the associated Poisson distribution. We’ll not spend much time on the details since such processes are not our primary focus, but a wonderful reading for them is <a class="citation" href="#feller1949theory">(Feller, 1949)</a>.</p><h3 id="diffusion-processes">Diffusion Processes</h3><p>Another possible continuous extension can be made in the case where the states of the system are real numbers (or vectors). In this case, we can formulate a sensible process if in an infinitesimally small time segment $\mathrm{d}t,$ the change in state is infinitesimal almost surely (recall that this change in state is stochastic). It is not immediately clear how to characterize such a transition function, but as we’ll see, it shows up quite naturally. In the next few sections we shall only consider a single-dimensional state space but the multi-dimensional case can be argued in a similar way with a bit more elbow grease.</p><h2 id="the-kolmogorov-equations">The Kolmogorov Equations</h2><p>To get a better sense of what the diffusion processes look like, we can start by writing similar equations for them as we did for the discrete case. Since now our state space is continuous, we will be dealing with probability densities instead of probabilities. Replacing the $\sum$ with a $\int$, the recursive equations from before become:</p>\[p(x;t|y;s) = \int_{-\infty}^{\infty} p(x;t|k;m)p(k;m|y;s)\mathrm dk\]<p>where the integral is understood to be over the entire state space. Here the notation has been simplified a bit such that $p(x;t|y;s)$ represents the probability density $p(X_t=x|X_s=y)$ with $t \geq s$ always. This notation more explicitly shows that this conditional density is a function of four variables $x, t, y$ and $s$.</p><p>As before, we have another equation that doesn’t depend on the Markov property:</p>\[p(x; t) = \int_{-\infty}^{\infty} p(x; t|k; m)p(k;m) \mathrm dk\]<p>We will use these equations to further probe the conditions required to come up with the “compensation” mentioned in the previous section. Before that, however, we can notice that the conditional probability density $p(x; t|y; s)$ and the initial marginal density $p(x; 0)$ should completely determine the entire diffusion process: given any collection of time points ${t_0, t_1, \dots, t_n}$, we can determine the joint density of the corresponding random variables (and hence characterize their distribution) using these two functions alone.</p><h3 id="the-forward-kolmogorov-equation">The Forward Kolmogorov Equation</h3><p>Even if the evolution of the state-trajectory of a system in a diffusion process is difficult to grasp because of the infinities and the infinitesimals at play, we can perhaps try to describe the evolution of the conditional probability density function $p(x; t|y; s)$.</p><p>Let us consider the evolution of the conditional density function over a very small time interval $\mathrm dt$, and apply the recursive equation from earlier to it:</p>\[p(x; t + \mathrm dt|y; s) = \int_{-\infty}^{\infty} p(x; t + \mathrm dt|m; t)p(m; t|y; s) \mathrm dm\]<p>Here, the first multiplicand within the integral deals with a transition from $m$ to $x$ over a very small time period $\mathrm dt$. We know that over such a small time period, with a very large probability, the change in the the value of $X_t$ will be very small. It is difficult to characterize this “probably very small” change, but we can reparametrize the equation in terms of it. Let us call this change $\Delta$. In particular, let us define:</p>\[\phi_t(\Delta; z) = p(z + \Delta; t + \mathrm dt | z; t)\]<p>Note how this is a distribution over $\Delta$. We can visualize this reparametrization like so:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/img/2021-11-11-sde/forward.jpg" alt="Forward Equation" width="40%" /></p><p>Based on this, we can rewrite the equation:</p>\[\begin {aligned} m &amp;= x - \Delta\\ \Rightarrow \mathrm dm &amp;= -\mathrm d\Delta\\ m = \pm \infty &amp;\Rightarrow \Delta = \mp \infty\\ \Rightarrow p(x; t + \mathrm dt|y; s) &amp;= -\int_{\infty}^{-\infty} \phi_t(\Delta; m)p(m; t|y; s) \mathrm d\Delta\\ &amp;= \int_{-\infty}^{+\infty} \phi_t(\Delta; m)p(m; t|y; s) \mathrm d\Delta \end{aligned}\]<p>We can now Taylor expand the the entire function within the integral with respect to $m$ around $x$:</p>\[\begin{aligned} p(x; t + \mathrm dt|y; s) &amp;= \int_{-\infty}^{+\infty} \phi_t(\Delta; x)p(x; t | y; s)\mathrm d\Delta \\ &amp;- \int_{-\infty}^{+\infty}\Delta\frac{\partial}{\partial x}\phi_t(\Delta; x)p(x; t|y; s)\mathrm d\Delta\\ &amp;+ \int_{-\infty}^{+\infty}\frac{\Delta^2}{2}\frac{\partial^2}{\partial x^2}\phi_t(\Delta; x)p(x; t|y; s) \mathrm d\Delta\\ &amp;\,\,\,\vdots \end{aligned}\]<p>We can then use the fact that $\phi_t$ integrates to $1$ over $\Delta$, and that we can swap the order of integration and partial differentiation and get:</p>\[\begin{aligned} p(x; t + \mathrm dt|y; s) - p(x; t|y; s) &amp;= -\frac{\partial}{\partial x}\left(\mathbb E_{\Delta \sim \phi_t(;x)}[\Delta] p(x;t|y;s)\right)\\ &amp; + \frac{1}{2}\frac{\partial^2}{\partial x^2}\left(\mathbb{E}_{\Delta \sim \phi_t(; x)}\left[\Delta^2\right]p(x;t|y;s)\right)\\ &amp;\,\,\,\vdots \end{aligned}\]<p>We now truncate the series to the second term, the justification for which will be presented in a bit. To get a sane limit on the right hand side, we must make sure that the first and the second moments of $\phi_t(;x)$ are of the order $\mathrm dt$. To that end, let us define:</p>\[\begin{aligned} \mathbb E_{\Delta \sim \phi_t(;x)}[\Delta] &amp;:= f(x, t)\mathrm dt\\ \mathbb E_{\Delta \sim \phi_t(; x)}\left[\Delta^2\right] &amp;:= g^2(x, t)\mathrm dt \end{aligned}\]<p>Dividing by $\mathrm dt$ on both the sides we get the partial differential equation:</p>\[\frac{\partial}{\partial t}p(x; t|y; s) = -\frac{\partial}{\partial x}\left( f(x, t)p(x;t|y;s)\right) + \frac{1}{2}\frac{\partial^2}{\partial x^2}\left(g^2(x; t)p(x;t|y;s)\right)\]<p>This is the Kolmogorov forward equation, also called the <a href="https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">Fokker-Planck equation.</a></p><p>Let us now try to justify, retroactively, why we truncated the Taylor series up to the second moment. The fact is, that for “nice” distributions which are concentrated over an infinitesimally small region (which we assume $\phi_t$ is), you’d expect $\mathbb E[\Delta^3] \ll O(\mathbb E[\Delta]) = O(\mathrm dt)$ and $\mathbb E[\Delta^4] \ll O(\mathbb E[\Delta^2]) = O(\mathrm dt)$, and so we can ignore them (and higher moments). But then the question arises, why isn’t the second moment much smaller than the first one, and therefore negligible compared to $\mathrm dt$? It’s because the second moment is unsigned, so depending on how symmetric the distribution is, it is possible (though not necessary) that it matches or exceeds the size of the first moment. For example, a symmetric distribution would have a $0$ first moment, but can have an $O(\mathrm dt)$ sized second moment. Therefore, it makes sense for us to consider the largest odd and the largest even moment.</p><p>Deriving this PDE has automatically brought forward the “compensation” we talked about in terms of moments of $\phi_t$. In fact, $\phi_t$ actually serves as a distribution for the change in $X$ in time $\mathrm dt$. This allows us to talk about the small changes in $X$:</p>\[\begin{aligned} \mathrm dX &amp;\sim \phi_t(; X)\\ \mathbb{E}[\mathrm dX] &amp;= f(X, t)\mathrm dt\\ \mathrm{Var}(\mathrm dX) &amp;= g^2(X, t)\mathrm dt - O(\mathrm dt^2) \approx g^2(X, t)\mathrm dt \end{aligned}\]<p>The function $f$ is called the <em>drift coefficient</em> of our diffusion process, and $g$ is called the <em>diffusion coefficient</em>. Based on this, we can separate out the location and the scale of the distribution of $\mathrm dX$ and write:</p>\[\mathrm dX = f(X, t)\mathrm dt + g(X, t)\mathrm dw\]<p>Where $\mathrm dw \sim D$ such that $D$ is some distribution with a variance of $\mathrm dt$ and a mean of $0$, and is independent of the current and past values of $X$. This equation is called the <a href="https://en.wikipedia.org/wiki/It%C3%B4_diffusion">Ito diffusion</a> SDE. I will now argue that we can always consider $D$ to be a Gaussian distribution, which will also tell us why the evolution of the probability distribution only seems to depend on the mean and the variance of the distribution but nothing else.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/img/2021-11-11-sde/junji_ito.jpg" alt="Junji Ito" title="Ito diffusion or something idk" width="30%" /></p><p>$\tiny{\text {someone please get this joke, i spent way too long on it}}$</p><p>Let us once again turn to the familiar land of deterministic functions. Suppose we are given a differential equation of the form</p>\[\mathrm dy = f(x,y)\mathrm dx\]<p>which we want to integrate to find $y(1)$ given the initial conditions $(0, y_0)$. How will we go about finding an approximate solution numerically? We can divide the interval $[0, 1]$ into $N$ pieces of width $\frac{1}{N}$ and assume that the value of $f$ is constant on each of these segments. Then, we can write the integral as:</p>\[\begin{aligned} \int_{y_0}^{y(1)} \mathrm dy &amp;\approx \sum_{i=1}^{N}f_i\int_{\frac{i-1}{N}}^{\frac{i}{N}}\mathrm dx\\ &amp;= \sum_{i=1}^{N}\frac{f_i}{N} \end{aligned}\]<p>where $f_i$ is the value of $f$ at the left edge of the $i\text{th}$ segment, and so, can be calculated as we go using the latest value of $y$. This is, in fact, <a href="https://en.wikipedia.org/wiki/Euler_method">Euler’s method for numerical integration.</a> It makes sense then, that for a well-behaved $f$, as we make $N$ larger and larger and therefore the segment length smaller and smaller, we will approach the true solution.</p><p>We will now apply the same heuristic to our diffusion equation. Suppose we want to “integrate” our stochastic equation from $t=0$ to $t=1$ (really, the limits can be anything but $0$ to $1$ makes exposition easier). We divide our time from $0$ to $1$ into $N$ segments, and assume that the values of $f$ and $g$ are constant within a segment and equal to the values at the left edge. We can then write the integral as:</p>\[\begin{aligned} \int_{X_0}^{X_1} \mathrm dX &amp;\approx \sum_{i=1}^{N}f_i\int_{\frac{i-1}{N}}^{\frac{i}{N}}\mathrm dt + \sum_{i=1}^{N}g_i\int_{\frac{i-1}{N}}^{\frac{i}{N}}\mathrm dw\\ &amp;= \sum_{i=1}^{N}\frac{f_i}{N} + \sum_{i=1}^{N}g_i\int_{\frac{i-1}{N}}^{\frac{i}{N}}\mathrm dw \end{aligned}\]<p>Notice how the $\mathrm dw$ has a hidden $\mathrm dt$ in it because it is a sample from a distribution with a zero mean and a variance of $\mathrm dt$. It is, however, not immediately clear how to simplify this “integral” any further.</p><p>Let us focus only on the integral in the second term, where we have the unwieldly $\mathrm dw$. The first thing to notice is that all the segments are independent since all $\mathrm dw$ are independent, so we can focus on only the first integral of $\mathrm dw$ from $0$ to $\frac{1}{N}$.</p><p>We can try discretizing this segment again into sub-segments and assume that in the $j\text{th}$ time sub-segment of length $\Delta t$, the change is a random variable $\Delta w_j$ with distribution $D$ with $0$ mean and $\Delta t$ variance. Then, intuitively, this “integral” can be represented as a sum, with the limit of $\Delta t$ tending to 0:</p>\[\int_{0}^{\frac{1}{N}}\mathrm dw = \lim_{\Delta t \rightarrow 0}\sum_{j=1}^{\frac{1}{N\Delta t}} \Delta w_j\]<p>Now, by the Central Limit Theorem, the sum of those random variables should approach a Gaussian as we add more and more of them. The variance of this Gaussian would be the sum of the variances of all of them and should therefore be equal to $\frac{1}{N}$, while the mean will be $0$. So, we get that the integral on the left is a random variable such that:</p>\[\int_{0}^{\frac{1}{N}} \mathrm dw \sim \mathcal N(0, \frac{1}{N})\]<p>Note that this is <em>not</em> an approximation, and we have resolved the limit here. (Also, the second parameter of the normal distribution will be the <strong>variance</strong> throughout this blog post.) This integral of $\mathrm dw$ is called the <a href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a> or the <a href="https://en.wikipedia.org/wiki/Wiener_process">Wiener process.</a></p><p>We can now plug this back into our discrete approximation of the Ito equation and get (the $\Delta w$’s here are, of course, different from the ones introduced previously):</p>\[\int_{X_0}^{X_1} \mathrm dX \approx \sum_{i=1}^{N}\frac{f_i}{N} + \sum_{i=1}^{N}g_i\Delta w_i\]<p>Here all of $\Delta w_i$ are independent Gaussian random variables with $0$ mean and $\frac{1}{N}$ variance. It makes sense, then, that as we increase $N$, this approximation will get tighter and tighter, and thus, $\mathrm dw$ can be treated as a $0$ mean Gaussian with variance $\mathrm dt \approx \frac{1}{N}$.</p><p>Let’s take a step back here. While arguing this, we have gone through <em>two</em> discretizations. Couldn’t we have gotten to the same point through just one? The fact is, we were able to convincingly resolve the first limit fully without any leftover approximations. This made arguing the second point much easier.</p><p>Finally, we can similarly argue about the unconditioned probability density evolution and get an unconditional forward equation (we just have to use the unconditioned recursive equation instead of the conditional one):</p>\[\frac{\partial}{\partial t}p(x; t) = -\frac{\partial}{\partial x}\left( f(x, t)p(x;t)\right) + \frac{1}{2}\frac{\partial^2}{\partial x^2}\left(g^2(x; t)p(x;t)\right)\]<h3 id="the-backward-equation">The Backward Equation</h3><p>The forward equation tells us how our conditional density evolves as the present time $t$ moves forward. However, what happens if we try to wiggle the time being conditioned upon? What happens if we condition on time $s - \mathrm ds$ instead of $s$? Indeed, the answer to that is an integral part of our recipe for reversing diffusion. The partial differential equation that that chronicles this wiggling is called the Kolmogorov “backward” equation. It describes the change in $p(x;t|y;s)$ with respect to $s$ (hence the term “backward”; recall that $t \geq s$). We can derive it in a manner very similar to the forward equation:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="../../assets/img/2021-11-11-sde/backward.jpg" alt="Backward Equation" width="40%" /></p><p>Based on this, we can write:</p>\[\begin{aligned} p(x;t|y;s-\mathrm ds) &amp;= \int_{-\infty}^{+\infty}\phi_{s - \mathrm ds}(\Delta; y)p(x;t|y + \Delta; s)\mathrm d\Delta\\ &amp;= \int_{-\infty}^{+\infty}\phi_{s - \mathrm ds}(\Delta; y)p(x;t|y;s)\mathrm d\Delta\\ &amp;+ \int_{-\infty}^{+\infty}\phi_{s - \mathrm ds}(\Delta; y)\Delta\frac{\partial}{\partial y}p(x;t|y;s)\mathrm d\Delta\\ &amp;+ \int_{-\infty}^{+\infty}\phi_{s - \mathrm ds}(\Delta; y)\frac{\Delta^2}{2}\frac{\partial^2}{\partial y^2}p(x;t|y;s)\mathrm d\Delta\\ &amp;\,\,\,\vdots \end{aligned}\]<p>We truncate to the second moment for reasons described earlier and then apply the definitions from before. Taking the limit $\mathrm ds \rightarrow 0$ we get:</p>\[-\frac{\partial}{\partial s}p(x;t|y;s) = f(y, s)\frac{\partial}{\partial y}p(x;t|y;s) + \frac{g^2(y; s)}{2}\frac{\partial^2}{\partial y^2}p(x;t|y;s)\]<p>which is the Kolmogorov backward equation.</p><p>We now have two (three if you count the unconditional forward equation as separate from the conditional one) partial differential equations for the probability densities associated with our diffusion, which we will now use to describe the reverse of the diffusion process.</p><h2 id="reversing-time">Reversing Time</h2><p>We shall now look at a remarkable result from <a class="citation" href="#anderson1982reverse">(Anderson, 1982)</a> which tells us how we can describe the reversal of a diffusion process. But first, what do we actually <em>mean</em> by this reversal?</p><p>Suppose you have an initial distribution $p(x; 0)$ and some drift and diffusion coefficients $f$ and $g$. Together, they describe a distribution over the <em>trajectories</em> of your system through time. Suppose at time $T$, the marginal distribution of the state is $p(x; T)$. Can you describe a diffusion process such that it starts with a distribution $q(x; 0) := p(x; T)$ and evolves such that the distribution of the trajectories is the same as in the original process but with the time reversed? Of course, in such a case we shall have $q(x; T) = p(x; 0)$.</p><p>Going back to the example of the diffusion models mentioned in the introduction (forget that they have discrete timesteps for a moment), we start with the data distribution $p(x; 0)$ and follow a diffusion process such that $p(x; T)$ is a Gaussian distribution. The reverse-time diffusion in this case would be a process which starts with a Gaussian distribution and ends up with the data distribution, with the distribution of the trajectories being mirrored in the time axis.</p><p>Let’s once again try to harness some intuition from the deterministic case. Suppose we have:</p>\[\begin{aligned} \mathrm dx &amp;= f(x, t)\mathrm dt\\ \Rightarrow x_T &amp;= x_0 + \int_{0}^{T}f(x, t)\mathrm dt\\ \Rightarrow x_0 &amp;= x_T - \int_{0}^{T}f(x, t) \mathrm dt \end{aligned}\]<p>Hmm, that seems rather easy. After all, we have:</p>\[-\mathrm dx = -f(x, t)\mathrm dt\]<p>Can’t we do something similar with our Ito SDE? It is also of the form:</p>\[\mathrm dX = f(X, t)\mathrm dt + g(X, t) \mathrm dw\]<p>Unfortunately, the deterministic variant doesn’t translate directly for us in this case. This is because in our SDE, the tiny Gaussian noise term $\mathrm dw$ is independent of the past values of $X$ but <em>not</em> of the future ones. So we can’t just slap a negative sign there and integrate from the future to the past by sampling $\mathrm dw$ independently of the values before it.</p><p>Thankfully, we can once again look at the flow of the probability densities. Since we know that the probability densities will remain the same in the reverse-time case except for the time-mirroring, all we need to do is get the Kolmogorov equations for $p(y;s|x;t)$ but with $s\leq t$. To do so, we can look at the <em>joint</em> density $p(x;t, y;s)$ and then try to convert it to the conditional form we desire. In particular, we will look at how it changes when we wiggle the $s$:</p>\[\begin{aligned} p(x;t, y;s) &amp;= p(y;s)p(x;t| y;s)\\ \Rightarrow \frac{\partial}{\partial s}p(x;t, y;s) &amp;= p(y;s)\frac{\partial}{\partial s}p(x;t|y;s) + p(x;t|y;s)\frac{\partial}{\partial s}p(y;s)\\ &amp;= -p(y;s)\left[f(y;s)\frac{\partial}{\partial y}\frac{p(x;t,y;s)}{p(y;s)} + \frac{g^2(y;s)}{2}\frac{\partial^2}{\partial y^2}\frac{p(x;t,y;s)}{p(y;s)}\right]\\ &amp; + \frac{p(x;t, y;s)}{p(y;s)}\left[-\frac{\partial}{\partial y}f(y;s)p(y;s) + \frac{1}{2}\frac{\partial^2}{\partial y^2}g^2(y;s)p(y;s)\right] \end{aligned}\]<p>where in the last step we have just applied the backward and the unconditional forward equations. Dividing both the sides with $p(x;t)$, which is essentially a constant when considering only changes with respect to $y$ and $s$, we get:</p>\[\begin{aligned} \frac{\partial}{\partial s}p(y;s|x;t) &amp;= -p(y;s)\left[f(y;s)\frac{\partial}{\partial y}\frac{p(y;s|x;t)}{p(y;s)} + \frac{g^2(y;s)}{2}\frac{\partial^2}{\partial y^2}\frac{p(y;s|x;t)}{p(y;s)}\right]\\ &amp; + \frac{p(y;s|x;t)}{p(y;s)}\left[-\frac{\partial}{\partial y}f(y;s)p(y;s) + \frac{1}{2}\frac{\partial^2}{\partial y^2}g^2(y;s)p(y;s)\right] \end{aligned}\]<p>The left hand side is all set, all we need to do is get the right hand side in the form of the forward Kolmogorov equation. In particular, we need to find drift and diffusion coefficients such that they are only a function of $x$ and $t$.</p><p>What follows is a rather filthy algebraic simplification and doesn’t seem to provide much insight. Feel free to skip it and go straight to the end, I just have it here for completion and also because <a class="citation" href="#anderson1982reverse">(Anderson, 1982)</a> skips the working (exercises for the reader amirite 🥲👍?). To make the notation cleaner, I will use $f_y$ to denote $\frac{\partial f}{\partial y}$ and $q$ and $p$ to denote $p(y;s|x;t)$ and $p(y;s)$ respectively. So it becomes:</p>\[\begin{aligned} q_s&amp;=-pf\left(\frac{q}{p}\right)_y - \frac{pg^2}{2}\left(\frac{q}{p}\right)_{yy} -\frac{q}{p}(fp)_y + \frac{q}{2p}(g^2p)_{yy} \end{aligned}\]<p>We combine the first and the third term into the derivative of a product, and then add and subtract</p><p>\(\frac{\left(pg^2\right)_y}{2}\left(\frac{q}{p}\right)_y\) and combine these two new terms with the third and the fourth terms:</p>\[\begin{aligned} q_s&amp;=-(fq)_y - \left(\frac{pg^2}{2}\left(\frac{q}{p}\right)_y\right)_y + \left(\frac{q}{2p}(g^2p)_y\right)_y \end{aligned}\]<p>Finally, we add and subtract a copy of the third term, and combine a bunch of terms into product derivatives again:</p>\[\begin{aligned} q_s&amp;=-\left(\left(f - \frac{1}{p}(g^2p)_y\right)q\right)_y - \frac{\left(g^2q\right)_{yy}}{2} \end{aligned}\]<p>Now, if we actually want to view the reverse process as a forward process with time index $u$, we should set $\mathrm ds = -\mathrm du$ so that $u$ ranges from $0$ to $T$ instead of $T$ to $0$. This change will help us compare with the form of the forward equation directly and extract the drift and the diffusion coefficients:</p>\[\begin{aligned} q_u = -\left(\left(\frac{1}{p}(g^2p)_y - f\right)q\right)_y + \frac{\left(g^2q\right)_{yy}}{2} \end{aligned}\]<p>We can now immediately recognize the drift and the diffusion parameters from it and write the Ito SDE for the reverse case:</p>\[\mathrm dX = \left(\frac{1}{p(X, T - u)}\frac{\partial}{\partial X}g^2(X, T - u)p(X, T - u) - f(X, T-u)\right)\mathrm du + g(X, T-u)\mathrm dw\]<p>Following the convention in all the cited papers, we will instead write it in terms of $\mathrm dt = - \mathrm du$ (so that the time goes from $T$ to $0$ in the reverse case) and get:</p>\[\mathrm dX = \left(f(X, t) -\frac{1}{p(X, t)}\frac{\partial}{\partial X}g^2(X, t)p(X, t)\right)\mathrm dt + g(X, t)\mathrm dw\]<p>The astute reader would’ve noticed that in that last step we have also silently replaced the Brownian motion with a time reversed version such that $\mathrm dw$ has a variance of $-\mathrm dt$ since $\mathrm dt$ itself is negative. The not-so-astute writer has decided to gloss over it. We can now get into the good stuff and see how this applies to diffusion models.</p><h2 id="diffusion-models-as-sdes">Diffusion Models as SDEs</h2><p>This section largely follows Appendix B of <a class="citation" href="#song2020score">(Song et al., 2020)</a>, so it might be a good idea to huff it straight from the source now that we have all the tools to understand it. Nevertheless, let’s keep moving forward.</p><p>First using good ol’ intuition, we generalize our results from the previous sections to the multidimensional case. However, we restrict the diffusion coefficient to be a scalar (or a scalar multiplied with the identity matrix) which only depends on the time $t$ and not $\mathbf X$. The forward SDE is:</p>\[\mathrm d\mathbf X = \mathbf f(\mathbf X, t)\mathrm dt + g(t)\mathrm d\mathbf w\]<p>Here $\mathbf f$ is a vector function and $\mathrm d\mathbf w$ is a sample from a spherical Gaussian with variance $\mathrm dt$ and with time ranging from $0$ to $T$. The reverse-time SDE is:</p>\[\begin{aligned} \mathrm d\mathbf X &amp;= \left(\mathbf f(\mathbf X, t) - \frac{g^2(t)}{p(\mathbf X, t)}\nabla_{\mathbf X} p(\mathbf X, t)\right)\mathrm dt + g(t)\mathrm d\mathbf w\\ &amp;= \left(\mathbf f(\mathbf X, t) - g^2(t)\nabla_{\mathbf X}\log p(\mathbf X, t)\right)\mathrm dt + g(t)\mathrm d\mathbf w \end{aligned}\]<p>Here time ranges from $T$ to $0$ and thus $\mathrm dt$ is a negative increment, and $\mathrm d\mathbf w$ is a sample from a spherical Gaussian with variance $-\mathrm dt$.</p><p>Let’s put this aside for a bit and look at how diffusion models are defined. Given a data distribution $p_{D}(\mathbf x)$ that we wish to learn, the diffusion model is defined as a discrete process such that:</p>\[\begin{aligned} \mathbf{x}_0 &amp;\sim p_{D}\\ \mathbf{x}_t &amp;= \sqrt{1 - \beta_t}\mathbf x_{t-1} + \sqrt{\beta_t}\mathbf\epsilon_t \,\forall t&gt;1 \end{aligned}\]<p>Here $\beta_t$ are some time dependent positive numbers less than $1$, and $\epsilon_t$ are standard spherical Gaussian noise terms independent of the past values of $\mathbf x$. The reverse process is then defined as:</p>\[\begin{aligned} \mathbf x_T &amp;\sim \mathcal N(0, 1)\\ \mathbf x_{t} &amp;= \mathbf \mu(\mathbf x_{t + 1}, t; \theta) + \Sigma(\mathbf x_{t + 1}, t; \theta)\gamma_t \,\forall t &lt; T \end{aligned}\]<p>Where $\mathbf \mu$ and $\mathbf \Sigma$ are neural networks which are trained such that the distribution of the forward trajectories matches the distribution of the reverse ones (see <a class="citation" href="#weng2021diffusion">(Weng, 2021)</a> for more details). $\gamma_t$ are standard spherical Gaussian noise terms independent of the future values of $\mathbf x$ (i.e. independent of $\mathbf x_u$ for $u &gt; t$)</p><p>These discrete processes are reminiscent of our continuous time diffusion processes. We will now try to get a continuous approximation for the forward discrete process and see if it aligns with an SDE. We can then get a reverse continuous time process for it and try to match it with the reverse discrete process prescribed by the diffusion models.</p><p>Indeed, if the $\beta_t$ are small enough and the number of steps are large enough, we can replace $\beta_t$ with an infinitesimal function $\beta(t)\mathrm dt$ such that at each step, instead of moving $1$ unit forward in time, we move $\mathrm dt$ units. With this approximation, we have:</p>\[\begin{aligned} \mathbf x_{t + \mathrm dt} - \mathbf x_{t} = \left(\sqrt{1 - \beta(t)\mathrm dt} - 1\right) \mathbf x_{t} + \sqrt{\beta(t)\mathrm dt}\epsilon(t) \end{aligned}\]<p>Using Taylor approximation to resolve the square root in the first term and recognizing that $\sqrt{\mathrm dt}\epsilon(t)$ is nothing but our good friend $\mathrm d\mathbf w$, we can write:</p>\[\mathrm d\mathbf x = -\frac{1}{2}\beta(t)\mathbf x\mathrm dt + \sqrt{\beta(t)}\mathrm d\mathbf w\]<p>and therefore we can defined the reverse process as:</p>\[\mathrm d\mathbf x = \left(-\frac{1}{2}\beta(t)\mathbf x - \beta(t)\nabla_x \log p(\mathrm x; t)\right)\mathrm dt + \sqrt{\beta(t)}\mathrm d\mathbf w\]<p>Discretizing this again, we see what exactly our $\mathbf \mu$ and $\mathbf \Sigma$ functions were trying to learn. In fact, we don’t even need to learn the $\Sigma$ function and we can just use the same coefficients as the forward process <a class="citation" href="#ho2020denoising">(Ho et al., 2020)</a>!</p><p>It is easy to see that given a <em>fixed</em> $\mathbf x_0$, our distribution at time $t$ will be <em>exactly</em> a Gaussian. That is, $p(\mathbf x(t) | \mathbf x(0))$ will be Gaussian. This is because even in the continuous case, we are just adding some Gaussian noise at every infinitesimal time step and we know that the sum of Gaussian random variables is Gaussian (you can rigorously prove it by studying the evolution of the density functions starting from a Dirac delta measure).</p><p>It also make sense that the <em>marginal</em> probability $p(\mathbf x(t))$ should resemble some sort of a normal distribution given enough time, since we keep scaling down the original data point and keep adding Gaussian noise. This tells us why we start our reverse process from a Gaussian distribution.</p><p>One remaining piece of the puzzle that we haven’t looked into yet is why at the end of the forward process, at time $T$, the mean of the Gaussian-like distribution should be $0$ and the variance $1$. We will now see that given a large enough $T$ and an appropriate $\beta$, we do eventually get there.</p><p>Let us try to see how the mean $\mathbb E[\mathbf x (t)] := \mu(t)$ varies as a function of time. It is fairly easy to see:</p>\[\begin{aligned} \frac{\mathrm d\mu(t)}{\mathrm dt} &amp;= \mathbb E\left[\frac{\mathrm d\mathbf x(t)}{\mathrm dt}\right]\\ &amp;= -\frac{1}{2}\beta(t)\mathbb \mu(t) \end{aligned}\]<p>Here we have exchanged expectation and differentiation since expectation is linear and used the fact that the expectation of $\mathrm d\mathbf w$ is $0$. This tells us that:</p>\[\mu(t) = \mu(0)\exp\left(-\frac{1}{2}\int_0^t\beta(t)\mathrm dt\right)\]<p>Thus, if our data distribution has an expectation of $\mu(0)$, it will eventually decay to $0$ if the integral of $\beta(t)$ goes to $\infty$ with time. Of course, conditioned on a fixed $\mathbf x(0)$ we have the conditional mean at time $t$ as</p>\[\mathbb E\left[\mathbf x(t) | \mathbf x(0)\right] = \mathbf x(0)\exp\left(-\frac{1}{2}\int_0^t\beta(t)\mathrm dt\right)\]<p>The case for variance is a bit more tricky because of certain peculiarities of Ito calculus which we have conveniently side-stepped so far. We have:</p>\[\begin{aligned} \Sigma(t) &amp;= \mathbb E\left[\mathbf x(t)\mathbf x(t)^T\right] - \mu(t)\mu(t)^T\\ \Rightarrow \frac{\mathrm d\Sigma(t)}{\mathrm dt} &amp;= \mathbb E\left[\frac{\mathrm d(\mathbf x(t)\mathbf x(t)^T)}{\mathrm dt}\right] + \beta(t)\mu(t)\mu(t)^T \end{aligned}\]<p>Unfortunately the chain rule doesn’t quite work as well in stochastic calculus, so we can’t differentiate some function of $\mathbf x$ like we usually do. This stems from the fact that the expectation of $\mathrm d \mathbf w^2$ is of the order $\mathrm dt$ and so not all quadratic terms disappear. Here the function in question, of course, is $\mathbf x(t)\mathbf x(t)^T$. Let’s do it the old fashioned way:</p>\[\begin{aligned} \mathrm d(\mathbf x(t)\mathbf x(t)^T) &amp;= [\mathbf x(t) + \mathrm d\mathbf x(t)][\mathbf x(t) + \mathrm d\mathbf x(t)]^T - \mathbf x(t)\mathbf x(t)^T\\ &amp;= -\beta(t) \mathbf x(t) \mathbf x(t)^T\mathrm dt + \frac{1}{4}\beta^2(t)\mathbf x(t)\mathbf x(t)^T\mathrm dt^2 -\frac{1}{2}\beta^{3/2}(t)\mathbf x(t)\mathrm d\mathbf w^Tdt\\ &amp;-\frac{1}{2}\beta^{3/2}(t)\mathrm d\mathbf w\mathbf x(t)^T\mathrm dt + \beta(t) \mathrm d\mathbf w\mathrm d\mathbf w^T \end{aligned}\]<p>Thankfully, this monstrosity simplifies once we take the expectation and divide by $\mathrm dt$, ignoring the $\mathrm dt^2$ term. We make use of the fact that $\mathrm d\mathbf w$ is independent of $\mathbf x(t)$ and has an expectation of $0$ and a covariance matrix of $\mathbf I\mathrm dt$ where $\mathbf I$ is the identity matrix. Combining all of this we get:</p>\[\begin{aligned} \frac{\mathrm d\Sigma(t)}{\mathrm dt} &amp;=\beta(t)(\mathbf I - \Sigma(t))\\ \Rightarrow \Sigma(t) &amp;= \mathbf I + \left(\Sigma(0) - \mathbf I\right)\exp\left(-\int_{0}^{t}\beta(t)\mathrm dt\right) \end{aligned}\]<p>which shows us that the covariance matrix goes to identity if, as in the case of expectation, $\beta(t)$ goes to $\infty$ with time. Of course, as before, conditioned on a fixed $\mathbf x(0)$, since we have $\Sigma(0) = 0$, we get:</p>\[\mathrm{var}(\mathbf x(t) |\mathbf x(0)) = \left(1 - \exp\left(-\int_0^t\beta(t)\mathrm dt\right)\right)\mathbf I\]<p>With this we have now seen that diffusion models are just discretizations of a particular SDE, and therefore have a corresponding reverse SDE as well. The reverse SDE then enlightened us about what exactly the neural networks in the diffusion models are trying to learn. Finally, we also justified the boundary condition of starting with a standard Gaussian distribution for the reverse diffusion process.</p><h2 id="score-matching">Score Matching</h2><p>Once we know what exactly the neural network $\mu(; \theta)$ is trying to learn, we can use that to our advantage. We see that all we need to learn to formulate the reverse process is the term $\nabla_\mathbf x \log p(\mathbf x; t)$. Once we have this, we can fully characterize the reverse process given $\beta(t)$.</p><p>Let us only consider a particular time instant $t$ where we want to learn a parametrized function (say, a neural network) $s(\mathbf x, t; \theta)$ (with $\theta$ as the set of parameters) that predicts $\nabla_x \log p(\mathbf x; t)$. Intuitively, we can try to minimize the squared error:</p>\[\mathcal{L_t(\theta)} = \mathbb{E}_{p(\mathbf x_t, t)}\left[\lVert s(\mathbf x_t, t; \theta) - \nabla_{\mathbf x_t}\log p(\mathbf x_t; t)\lVert^2\right]\]<p>This function (if we ignore all the $t$’s) is called the Fisher divergence between the learned distribution and the actual distribution. Minimizing this loss function is called “score matching” and is a well studied problem <a class="citation" href="#hyvarinen2005estimation">(Hyvärinen &amp; Dayan, 2005)</a>. It is fairly easy to show that if two distributions agree on the gradient of the log-probability densities, they should be the same almost everywhere (line-integrate both the sides between two points and use the fact that probability densities integrate to $1$). Score matching may not always produce the same results as a maximum likelihood estimate, which minimizes the KL-divergence, but has some nicer stability properties when faced with noisy data <a class="citation" href="#lyu2012interpretation">(Lyu, 2012)</a>. It is also much more wieldy for high-dimensional distributions since we do not have to calculate the <a href="https://en.wikipedia.org/wiki/Normalizing_constant">normalization constant</a> for the density function since it is separated out because of the logarithm and then eradicated by the gradient operation.</p><p>As an aside, the score of a distribution is <a href="https://en.wikipedia.org/wiki/Score_(statistics)">typically defined</a> as the gradient of the log-density function with respect to a <em>parameter</em> and <em>not</em> the variable, which makes the “score matching” terminology particularly confusing. So, for example, the score of a Gaussian density $\mathcal N(x; \mu, 1)$ parametrized by $\mu$ will be defined as:</p>\[\text{score}(\mu) = \frac{\partial \log \mathcal N(x; \mu, 1)}{\partial \mu} = x - \mu\]<p>You can twist this definition a bit to achieve our definition by parametrizing the distribution with an additional “location” parameter $l$ such that: \(q(x; l) := p(x + l)\)</p><p>We can then calculate the score of $q$ at $l=0$ and reach our definition of “score”:</p>\[\nabla_l\log q(x; l)|_{l=0} = \nabla_l\log p(x + l)|_{l=0} = \nabla_x\log p(x)\]<p>However, this arm-twisting doesn’t seem to yield any groundbreaking insights.</p><p>Going back to $\mathcal L_t(\theta)$, our dataset comprises samples from $p(\mathbf x; 0)$. We can easily get samples from $p(\mathbf x; t)$ by simulating the diffusion process, thereby getting a Monte Carlo estimate of the expectation. However, what we don’t have is the actual score function $\nabla_{\mathbf x_t}\log p(\mathbf x_t; t)$ to minimize the error against. Instead, we use the trick proposed in <a class="citation" href="#vincent2011connection">(Vincent, 2011)</a> called “Denoising Score Matching.” The idea is to somehow rewrite this loss function so that we have a gradient of the conditional log-density $\log p(\mathbf x_t, t| \mathbf x_0, 0)$ instead of the marginal one. As we saw in the last section, we know that this conditional density will be a Gaussian with some pre-determined mean and variance, so computing it should be fairly straightforward.</p><p>The following is mostly a reproduction of the appendix of that paper in our context. We have:</p>\[\begin{aligned} \mathcal L_t(\theta) &amp;= \mathbb{E}_{p(\mathbf x_t, t)}[\lVert s(\mathbf x_t, t; \theta) - \nabla_{\mathbf x_t}\log p(\mathbf x_t; t)\lVert^2]\\ &amp;= \mathbb{E}_{p(\mathbf x_t, t)}\left[\lVert s(\mathbf x_t, t; \theta)\lVert^2\right] -2\mathbb{E}_{p(\mathbf x_t, t)}\left[s(\mathbf x, t; \theta)^T\nabla_{\mathbf x_t}\log p(\mathbf x_t; t)\right]\\ &amp; + \mathbb{E}_{p(\mathbf x_t, t)}\left[\lVert\nabla_{\mathbf x_t}\log p(\mathbf x_t; t)\lVert^2\right] \end{aligned}\]<p>Here the last term does not depend on $\theta$, so we can ignore it and only work with the first two terms. We will now use the following simple consequence of the chain rule:</p>\[\mathbb E_{p}\left[f(\mathbf x)\nabla_x \log p(\mathbf x)\right] = \int f(\mathbf x)\nabla_xp(\mathbf x)\mathrm dV\]<p>If we only look at the second term in $\mathcal L_t$, we see:</p>\[\begin{aligned} \mathbb{E}_{p(\mathbf x_t, t)}\left[s(\mathbf x, t; \theta)^T\nabla_{\mathbf x_t}\log p(\mathbf x_t; t)\right]&amp;= \int s(\mathbf x, t; \theta)^T\nabla_{\mathbf x_t}p(\mathbf x_t; t)\mathrm dV\\ &amp;= \int\mathbb E_{p(\mathbf x_0; 0)}\left[s(\mathbf x, t; \theta)^T\nabla_{\mathbf x_t}p(\mathbf x_t; t|\mathbf x_0; 0)\mathrm dV\right] \\ &amp;=\mathbb E_{p(\mathbf x_0; 0)}\mathbb E_{p(\mathbf x_t; t|\mathbf x_0, 0)}\left[s(\mathbf x, t; \theta)^T \nabla_{\mathbf x_t}\log p(\mathbf x_t; t| \mathbf x_0; 0)\right] \end{aligned}\]<p>Plugging this back into $\mathcal L_t$ and adding and subtracting $\mathbb E_{p(\mathbf x_0; 0)}\mathbb E_{p(\mathbf x_t; t)}\lVert \nabla_{\mathbf x_t}\log p(\mathbf x_t; t| \mathbf x_0; 0) \lVert^2$, which is independent of $\theta$, we get:</p>\[\begin{aligned} \mathcal L_t(\theta) &amp;= \mathbb E_{p(\mathbf x_0; 0)}\mathbb E_{p(\mathbf x_t; t|\mathbf x_0, 0)}\left[\lVert s(\mathbf x_t, t; \theta) - \nabla_{\mathbf x_t}\log p(\mathbf x_t; t|\mathbf x_0, 0) \lVert^2\right] + C\\ &amp;\equiv \mathbb E_{p(\mathbf x_0; 0)}\mathbb E_{p(\mathbf x_t; t|\mathbf x_0, 0)}\left[\lVert s(\mathbf x_t, t; \theta) - \nabla_{\mathbf x_t}\log p(\mathbf x_t; t|\mathbf x_0, 0) \lVert^2\right] \end{aligned}\]<p>Of course, so far we have only focused on the Fischer divergence for a particular time instant $t$. Ideally, we would like to match it almost everywhere from $0$ to $T$. So we instead want to minimize:</p>\[\mathcal L(\theta) = \int_{0}^{T}\mathcal L_t(\theta)\mathrm dt\]<p>We can rewrite this integral as an <a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">expectation over a uniform distribution,</a> and also add a <a href="https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem#Radon%E2%80%93Nikodym_derivative">positive weighting function</a> $\lambda(t)$ if we want to focus on certain time instants more than the others. This finally gets us to the loss function mentioned in <a class="citation" href="#song2020score">(Song et al., 2020)</a>:</p>\[\mathcal L(\theta) = \mathbb E_{t \sim \mathrm U[0, T]}\mathbb E_{p(\mathbf x_0; 0)}\mathbb E_{p(\mathbf x_t; t|\mathbf x_0, 0)}\left[\lambda(t)\lVert s(\mathbf x_t, t; \theta) - \nabla_{\mathbf x_t}\log p(\mathbf x_t; t|\mathbf x_0, 0) \lVert^2\right]\]<p>This loss is easy to get an estimate for. First we sample a time $t$ uniformly from $0$ to $T$. We then get a sample from our dataset, and then get a sample at time $t$ conditioned on our data point. This is easy to do since we know the conditional distribution is a Gaussian with the mean and variance as described earlier. We can then calculate the weighted squared error between the gradient of this Gaussian log-conditional density and our model prediction. This gives us an unbiased estimator which we can optimize using some flavor of gradient descent.</p><h2 id="tldr">Tl;dr</h2><p>Diffusion models are discretizations of a continuous-time stochastic differential equation. Looking at the time-reversal of this diffusion process tells us that the neural network in the diffusion model is actually trying to learn the gradient of the probability density function with respect to the variable at different time instants. This itself is a well-studied problem called score matching.</p><p>Fin.</p><hr /><h2 id="references">References</h2><ol class="bibliography"><li><span id="sohl2015deep">Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., &amp; Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. <i>International Conference on Machine Learning</i>, 2256–2265.</span><li><span id="ho2020denoising">Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. <i>Advances in Neural Information Processing Systems</i>, <i>33</i>, 6840–6851.</span><li><span id="dhariwal2021diffusion">Dhariwal, P., &amp; Nichol, A. (2021). Diffusion models beat gans on image synthesis. <i>Advances in Neural Information Processing Systems</i>, <i>34</i>.</span><li><span id="baranchuk2021label">Baranchuk, D., Rubachev, I., Voynov, A., Khrulkov, V., &amp; Babenko, A. (2021). Label-Efficient Semantic Segmentation with Diffusion Models. <i>ArXiv Preprint ArXiv:2112.03126</i>.</span><li><span id="weng2021diffusion">Weng, L. (2021). What are diffusion models? <i>Lilianweng.github.io/Lil-Log</i>. https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html</span><li><span id="song2020score">Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2020). Score-based generative modeling through stochastic differential equations. <i>ArXiv Preprint ArXiv:2011.13456</i>.</span><li><span id="feller1949theory">Feller, W. (1949). On the theory of stochastic processes, with particular reference to applications. <i>Proceedings of the [First] Berkeley Symposium on Mathematical Statistics and Probability</i>, 403–432.</span><li><span id="anderson1982reverse">Anderson, B. D. O. (1982). Reverse-time diffusion equation models. <i>Stochastic Processes and Their Applications</i>, <i>12</i>(3), 313–326.</span><li><span id="hyvarinen2005estimation">Hyvärinen, A., &amp; Dayan, P. (2005). Estimation of non-normalized statistical models by score matching. <i>Journal of Machine Learning Research</i>, <i>6</i>(4).</span><li><span id="lyu2012interpretation">Lyu, S. (2012). Interpretation and generalization of score matching. <i>ArXiv Preprint ArXiv:1205.2629</i>.</span><li><span id="vincent2011connection">Vincent, P. (2011). A connection between score matching and denoising autoencoders. <i>Neural Computation</i>, <i>23</i>(7), 1661–1674.</span></ol></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/machine-learning/'>Machine Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/diffusion/" class="post-tag no-text-decoration" >diffusion</a> <a href="/tags/ddpm/" class="post-tag no-text-decoration" >ddpm</a> <a href="/tags/ito-diffusion/" class="post-tag no-text-decoration" >ito diffusion</a> <a href="/tags/stochastic-differential-equations/" class="post-tag no-text-decoration" >stochastic differential equations</a> <a href="/tags/neural-sde/" class="post-tag no-text-decoration" >neural-sde</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Stochastic Differential Equations and Diffusion Models - VanillaBug&url=https://www.vanillabug.com/posts/sde/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Stochastic Differential Equations and Diffusion Models - VanillaBug&u=https://www.vanillabug.com/posts/sde/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Stochastic Differential Equations and Diffusion Models - VanillaBug&url=https://www.vanillabug.com/posts/sde/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/countability/">countability</a> <a class="post-tag" href="/tags/ddpm/">ddpm</a> <a class="post-tag" href="/tags/diffusion/">diffusion</a> <a class="post-tag" href="/tags/duality/">duality</a> <a class="post-tag" href="/tags/gan/">gan</a> <a class="post-tag" href="/tags/introduction/">introduction</a> <a class="post-tag" href="/tags/ito-diffusion/">ito diffusion</a> <a class="post-tag" href="/tags/kantorovich/">kantorovich</a> <a class="post-tag" href="/tags/natural-numbers/">natural numbers</a> <a class="post-tag" href="/tags/neural-sde/">neural-sde</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/wasserstein/"><div class="card-body"> <span class="timeago small" > Aug 28, 2020 <i class="unloaded">2020-08-28T09:31:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>The Kantorovich-Rubinstein Duality</h3><div class="text-muted small"><p> In this post we’ll talk about the Wasserstein-1 distance, which is a metric on the space of probability distributions, and the Kantorovich-Rubinstein duality, which establishes an elegant and rathe...</p></div></div></a></div><div class="card"> <a href="/posts/lets-count/"><div class="card-body"> <span class="timeago small" > Apr 21, 2020 <i class="unloaded">2020-04-21T13:19:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Let's Count!</h3><div class="text-muted small"><p> For the first post, let’s talk about something so innate to our intuition, that we might as well call it… natural. You guessed it, today we’ll talk about natural numbers and the rather odd notions ...</p></div></div></a></div><div class="card"> <a href="/posts/git-init/"><div class="card-body"> <span class="timeago small" > Apr 19, 2020 <i class="unloaded">2020-04-19T03:52:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>git init</h3><div class="text-muted small"><p> Freshly graduated with a master’s degree in computer science and basking in the newly discovered privilege of not having seventeen and a half deadlines every week, I’ve decided to try my hand at no...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/wasserstein/" class="btn btn-outline-primary" prompt="Older"><p>The Kantorovich-Rubinstein Duality</p></a> <span class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></span></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/many-facedgod">Tanmaya Shekhar Dabral</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/countability/">countability</a> <a class="post-tag" href="/tags/ddpm/">ddpm</a> <a class="post-tag" href="/tags/diffusion/">diffusion</a> <a class="post-tag" href="/tags/duality/">duality</a> <a class="post-tag" href="/tags/gan/">gan</a> <a class="post-tag" href="/tags/introduction/">introduction</a> <a class="post-tag" href="/tags/ito-diffusion/">ito diffusion</a> <a class="post-tag" href="/tags/kantorovich/">kantorovich</a> <a class="post-tag" href="/tags/natural-numbers/">natural numbers</a> <a class="post-tag" href="/tags/neural-sde/">neural sde</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://www.vanillabug.com{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
