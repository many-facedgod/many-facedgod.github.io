<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://www.vanillabug.com/</id><title>VanillaBug</title><subtitle>A blog for computer science, math and all things cool.</subtitle> <updated>2024-01-29T22:44:21-05:00</updated> <author> <name>Tanmaya Shekhar Dabral</name> <uri>https://www.vanillabug.com/</uri> </author><link rel="self" type="application/atom+xml" href="https://www.vanillabug.com/feed.xml"/><link rel="alternate" type="text/html" hreflang="en-US" href="https://www.vanillabug.com/"/> <generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator> <rights> © 2024 Tanmaya Shekhar Dabral </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>Stochastic Differential Equations and Diffusion Models</title><link href="https://www.vanillabug.com/posts/sde/" rel="alternate" type="text/html" title="Stochastic Differential Equations and Diffusion Models" /><published>2021-09-28T08:31:00-04:00</published> <updated>2021-09-28T08:31:00-04:00</updated> <id>https://www.vanillabug.com/posts/sde/</id> <content src="https://www.vanillabug.com/posts/sde/" /> <author> <name>Tanmaya Shekhar Dabral</name> </author> <category term="Machine Learning" /> <summary> Diffusion models (Sohl-Dickstein et al., 2015)(Ho et al., 2020) are one of the freshest flavors of generative models in the market right now (at least as of writing this post). They have been shown to outperform GANs in certain settings (Dhariwal &amp;amp; Nichol, 2021), and once trained, can also be used as feature extractors for supervised tasks (Baranchuk et al., 2021). As we shall see, they are... </summary> </entry> <entry><title>The Kantorovich-Rubinstein Duality</title><link href="https://www.vanillabug.com/posts/wasserstein/" rel="alternate" type="text/html" title="The Kantorovich-Rubinstein Duality" /><published>2020-08-28T09:31:00-04:00</published> <updated>2020-08-28T09:31:00-04:00</updated> <id>https://www.vanillabug.com/posts/wasserstein/</id> <content src="https://www.vanillabug.com/posts/wasserstein/" /> <author> <name>Tanmaya Shekhar Dabral</name> </author> <category term="Math" /> <summary> In this post we’ll talk about the Wasserstein-1 distance, which is a metric on the space of probability distributions, and the Kantorovich-Rubinstein duality, which establishes an elegant and rather useful dual for it. One of the many good things about this metric is that in many cases it yields nicer gradients when compared to other divergences like the Jensen-Shannon divergence. For this reas... </summary> </entry> <entry><title>Let's Count!</title><link href="https://www.vanillabug.com/posts/lets-count/" rel="alternate" type="text/html" title="Let's Count!" /><published>2020-04-21T13:19:00-04:00</published> <updated>2020-04-21T13:19:00-04:00</updated> <id>https://www.vanillabug.com/posts/lets-count/</id> <content src="https://www.vanillabug.com/posts/lets-count/" /> <author> <name>Tanmaya Shekhar Dabral</name> </author> <category term="Math" /> <summary> For the first post, let’s talk about something so innate to our intuition, that we might as well call it… natural. You guessed it, today we’ll talk about natural numbers and the rather odd notions surrounding them. The set of all natural numbers, $\mathbb{N} = \{0, 1, 2, …\}$, of course, goes on forever. If you give me a natural number, I can always produce the next one by adding $1$. And yet,... </summary> </entry> <entry><title>git init</title><link href="https://www.vanillabug.com/posts/git-init/" rel="alternate" type="text/html" title="git init" /><published>2020-04-19T03:52:00-04:00</published> <updated>2020-04-19T03:52:00-04:00</updated> <id>https://www.vanillabug.com/posts/git-init/</id> <content src="https://www.vanillabug.com/posts/git-init/" /> <author> <name>Tanmaya Shekhar Dabral</name> </author> <category term="Meta" /> <summary> Freshly graduated with a master’s degree in computer science and basking in the newly discovered privilege of not having seventeen and a half deadlines every week, I’ve decided to try my hand at non-fiction writing about things I may or may not know a bit about. And so, this blog will serve as a dumping ground for such ramblings, with the topics potentially ranging from fine arts, math and comp... </summary> </entry> </feed>
